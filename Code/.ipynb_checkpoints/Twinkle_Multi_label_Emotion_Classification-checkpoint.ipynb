{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer \n",
    "import re \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pathlib.Path.cwd() / 'train.tsv'\n",
    "df = pd.read_csv(dataset, sep='\\t', header=None, names=['comment', 'label', 'id'])\n",
    "df['label'] = df['label'].str.split(',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Mapping Label</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan mapping 27 label menjadi 7 label\n",
    "emotion_list = ['admiration', 'amusement', 'anger', 'annoyance', 'approval', 'caring', 'confusion', 'curiosity', 'desire', 'disappointment',                     \n",
    "                'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear', 'gratitude', 'grief', 'joy', 'love', 'nervousness', 'optimism',                 \n",
    "                'pride', 'realization', 'relief', 'remorse', 'sadness', 'surprise', 'neutral']\n",
    "\n",
    "enkman_mapping = {\n",
    "        \"anger\": [\"anger\", \"annoyance\", \"disapproval\"],\n",
    "        \"disgust\": [\"disgust\"],\n",
    "        \"fear\": [\"fear\", \"nervousness\"],\n",
    "        \"joy\": [\"joy\", \"amusement\", \"approval\", \"excitement\", \"gratitude\",  \"love\", \"optimism\", \"relief\", \"pride\", \"admiration\", \"desire\", \"caring\"],\n",
    "        \"sadness\": [\"sadness\", \"disappointment\", \"embarrassment\", \"grief\",  \"remorse\"],\n",
    "        \"surprise\": [\"surprise\", \"realization\", \"confusion\", \"curiosity\"],\n",
    "        \"neutral\": [\"neutral\"],\n",
    "        }\n",
    "enkman_mapping_rev = {v:key for key, value in enkman_mapping.items() for v in value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function from Google Research analysis \n",
    "def idx2class(idx_list):\n",
    "    arr = []\n",
    "    for i in idx_list:\n",
    "        arr.append(emotion_list[int(i)])\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emotion label to the label ids\n",
    "df['emotions'] = df['label'].apply(idx2class)\n",
    "\n",
    "# use enkman mapping to reduce the emotions to a list of ['anger', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'neutral']\n",
    "df['mapped_emotions'] = df['emotions'].apply(lambda x: [enkman_mapping_rev[i] for i in x])\n",
    "\n",
    "# fix issues where ['joy',' joy'] might appear\n",
    "df.loc[df['mapped_emotions'].apply(len)>1, 'mapped_emotions'] = df.loc[df['mapped_emotions'].apply(len)>1, 'mapped_emotions'].apply(lambda x: [emotion for emotion in set(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\rifka\\anaconda3\\lib\\site-packages (1.6.1)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import json\n",
    "!pip install emoji\n",
    "import emoji\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n",
    "                       \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                       \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n",
    "                       \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\",\n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \n",
    "                       \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n",
    "                       \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \n",
    "                       \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                       \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\",\n",
    "                       \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                       \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\",\n",
    "                       \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\",\n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\",\n",
    "                       \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                       \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\",\n",
    "                       \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}\n",
    "\n",
    "punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\",\n",
    "                 \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', \n",
    "                 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '!':' '}\n",
    "\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can',\n",
    "                'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', \n",
    "                'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', \n",
    "                'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', \n",
    "                'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization',\n",
    "                'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stopword_list = stopwords.words('english')\n",
    "\n",
    "def clean_text(text):\n",
    "    '''Clean emoji, Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
    "    and remove words containing numbers.'''\n",
    "    text = emoji.demojize(text)\n",
    "    text = re.sub(r'\\:(.*?)\\:','',text)\n",
    "    text = str(text).lower()    #Making Text Lowercase\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    #The next 2 lines remove html text\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\", \"'\")\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿']+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_contractions(text, mapping):\n",
    "    '''Clean contraction using contraction mapping'''    \n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    for word in mapping.keys():\n",
    "        if \"\"+word+\"\" in text:\n",
    "            text = text.replace(\"\"+word+\"\", \"\"+mapping[word]+\"\")\n",
    "    #Remove Punctuations\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    return text\n",
    "\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    '''Cleans special characters present(if any)'''   \n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  \n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "\n",
    "def correct_spelling(x, dic):\n",
    "    '''Corrects common spelling errors'''   \n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "\n",
    "def remove_space(text):\n",
    "    '''Removes awkward spaces'''   \n",
    "    #Removes awkward spaces \n",
    "    text = text.strip()\n",
    "    text = text.split()\n",
    "    return \" \".join(text)\n",
    "\n",
    "def tokenize_stem_no_stopwords(text):\n",
    "    return [stemmer.stem(w) for w in word_tokenize(text) if w not in stopword_list]\n",
    "\n",
    "def text_preprocessing_pipeline(text):\n",
    "    '''Cleaning and parsing the text.'''\n",
    "    text = clean_text(text)\n",
    "    text = clean_contractions(text, contraction_mapping)\n",
    "    text = clean_special_chars(text, punct, punct_mapping)\n",
    "    text = correct_spelling(text, mispell_dict)\n",
    "    text = remove_space(text)\n",
    "    text = tokenize_stem_no_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_comment'] = df['comment'].str.lower()\n",
    "df['processed_comment'] = df['processed_comment'].apply(text_preprocessing_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = df.shape[0]\n",
    "for emotion in enkman_mapping.keys():\n",
    "    df[emotion] = np.zeros((N,1), dtype=int)\n",
    "\n",
    "for emotion in enkman_mapping.keys():\n",
    "    df[emotion] = df['mapped_emotions'].apply(lambda x: 1 if emotion in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, random_state=156, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "\n",
    "x_train = tfidf.fit_transform(X_train['processed_comment'].apply(lambda x: ' '.join(x)))\n",
    "x_test = tfidf.transform(X_test['processed_comment'].apply(lambda x: ' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>emotions</th>\n",
       "      <th>mapped_emotions</th>\n",
       "      <th>processed_comment</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16737</th>\n",
       "      <td>It’s time to stop lad, it’s getting a little e...</td>\n",
       "      <td>[12]</td>\n",
       "      <td>ef64or1</td>\n",
       "      <td>[embarrassment]</td>\n",
       "      <td>[sadness]</td>\n",
       "      <td>[time, stop, lad, get, littl, embarrass]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>So glad my mom didn’t act this way when I was ...</td>\n",
       "      <td>[0, 17]</td>\n",
       "      <td>ediz2sg</td>\n",
       "      <td>[admiration, joy]</td>\n",
       "      <td>[joy]</td>\n",
       "      <td>[glad, mom, act, way, diagnos, oh, mom, smart,...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20213</th>\n",
       "      <td>Hopefully that's patched soon.</td>\n",
       "      <td>[20]</td>\n",
       "      <td>edmb4z7</td>\n",
       "      <td>[optimism]</td>\n",
       "      <td>[joy]</td>\n",
       "      <td>[hope, patch, soon]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>If you want a one minute story told in half an...</td>\n",
       "      <td>[1]</td>\n",
       "      <td>eedcfbk</td>\n",
       "      <td>[amusement]</td>\n",
       "      <td>[joy]</td>\n",
       "      <td>[want, one, minut, stori, told, half, hour, as...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29277</th>\n",
       "      <td>[NAME] has 6 eyes and will thus be the best ac...</td>\n",
       "      <td>[27]</td>\n",
       "      <td>edki307</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[neutral]</td>\n",
       "      <td>[eye, thu, best, actor, movi]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 comment    label       id  \\\n",
       "16737  It’s time to stop lad, it’s getting a little e...     [12]  ef64or1   \n",
       "5935   So glad my mom didn’t act this way when I was ...  [0, 17]  ediz2sg   \n",
       "20213                     Hopefully that's patched soon.     [20]  edmb4z7   \n",
       "1147   If you want a one minute story told in half an...      [1]  eedcfbk   \n",
       "29277  [NAME] has 6 eyes and will thus be the best ac...     [27]  edki307   \n",
       "\n",
       "                emotions mapped_emotions  \\\n",
       "16737    [embarrassment]       [sadness]   \n",
       "5935   [admiration, joy]           [joy]   \n",
       "20213         [optimism]           [joy]   \n",
       "1147         [amusement]           [joy]   \n",
       "29277          [neutral]       [neutral]   \n",
       "\n",
       "                                       processed_comment  anger  disgust  \\\n",
       "16737           [time, stop, lad, get, littl, embarrass]      0        0   \n",
       "5935   [glad, mom, act, way, diagnos, oh, mom, smart,...      0        0   \n",
       "20213                                [hope, patch, soon]      0        0   \n",
       "1147   [want, one, minut, stori, told, half, hour, as...      0        0   \n",
       "29277                      [eye, thu, best, actor, movi]      0        0   \n",
       "\n",
       "       fear  joy  sadness  surprise  neutral  \n",
       "16737     0    0        1         0        0  \n",
       "5935      0    1        0         0        0  \n",
       "20213     0    1        0         0        0  \n",
       "1147      0    1        0         0        0  \n",
       "29277     0    0        0         0        1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = X_train.sample(frac =0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data['comment'].values\n",
    "X= X.astype(str)\n",
    "y = data.iloc[:,6:].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "# !pip install --user keras\n",
    "# !pip install --user tensorflow\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,LSTM,Dropout, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "tokenizer = Tokenizer() #make a tokenizer\n",
    "tokenizer.fit_on_texts(X_train) #fit on text\n",
    "\n",
    "\n",
    "num_words=7000\n",
    "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= num_words} # <= because tokenizer is 1 indexed\n",
    "tokenizer.word_index[tokenizer.oov_token] = num_words + 1\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "maxlen = 20\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding accuracy: 0.8961725221365324\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word]\n",
    "                embedding_matrix[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix\n",
    "    \n",
    "embedding_dim = 300\n",
    "embedding_matrix = create_embedding_matrix('glove.6B.300d.txt', tokenizer.word_index, embedding_dim)\n",
    "\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "embedding_accuracy = nonzero_elements / vocab_size\n",
    "print('embedding accuracy: ' + str(embedding_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Model Building</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 300)           2100600   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 512)               1140736   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 903       \n",
      "=================================================================\n",
      "Total params: 3,307,903\n",
      "Trainable params: 3,307,903\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "    \n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.50))\n",
    "model.add(Dense(7, activation='softmax'))\n",
    "# Adam Optimiser\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "46/46 [==============================] - 54s 686ms/step - loss: 0.4119 - accuracy: 0.3755 - val_loss: 0.3347 - val_accuracy: 0.4846\n",
      "Epoch 2/15\n",
      "46/46 [==============================] - 31s 667ms/step - loss: 0.3355 - accuracy: 0.4981 - val_loss: 0.3083 - val_accuracy: 0.5179\n",
      "Epoch 3/15\n",
      "46/46 [==============================] - 30s 643ms/step - loss: 0.2983 - accuracy: 0.5580 - val_loss: 0.2957 - val_accuracy: 0.5358\n",
      "Epoch 4/15\n",
      "46/46 [==============================] - 27s 588ms/step - loss: 0.2665 - accuracy: 0.6210 - val_loss: 0.2881 - val_accuracy: 0.5440\n",
      "Epoch 5/15\n",
      "46/46 [==============================] - 27s 578ms/step - loss: 0.2370 - accuracy: 0.6759 - val_loss: 0.2938 - val_accuracy: 0.5686\n",
      "Epoch 6/15\n",
      "46/46 [==============================] - 27s 586ms/step - loss: 0.2048 - accuracy: 0.7296 - val_loss: 0.3090 - val_accuracy: 0.5660\n",
      "Epoch 7/15\n",
      "46/46 [==============================] - 28s 604ms/step - loss: 0.1714 - accuracy: 0.7759 - val_loss: 0.3390 - val_accuracy: 0.5629\n",
      "Epoch 8/15\n",
      "46/46 [==============================] - 29s 627ms/step - loss: 0.1459 - accuracy: 0.8106 - val_loss: 0.3455 - val_accuracy: 0.5502\n",
      "Epoch 9/15\n",
      "46/46 [==============================] - 28s 612ms/step - loss: 0.1249 - accuracy: 0.8374 - val_loss: 0.3779 - val_accuracy: 0.5517\n",
      "Epoch 10/15\n",
      "46/46 [==============================] - 28s 600ms/step - loss: 0.1033 - accuracy: 0.8699 - val_loss: 0.4066 - val_accuracy: 0.5338\n",
      "Epoch 11/15\n",
      "46/46 [==============================] - 30s 660ms/step - loss: 0.0916 - accuracy: 0.8800 - val_loss: 0.4043 - val_accuracy: 0.5358\n",
      "Epoch 12/15\n",
      "46/46 [==============================] - 30s 656ms/step - loss: 0.0759 - accuracy: 0.9015 - val_loss: 0.4496 - val_accuracy: 0.5312\n",
      "Epoch 13/15\n",
      "46/46 [==============================] - 31s 677ms/step - loss: 0.0646 - accuracy: 0.9146 - val_loss: 0.5065 - val_accuracy: 0.5399\n",
      "Epoch 14/15\n",
      "46/46 [==============================] - 28s 600ms/step - loss: 0.0549 - accuracy: 0.9234 - val_loss: 0.5450 - val_accuracy: 0.5343\n",
      "Epoch 15/15\n",
      "46/46 [==============================] - 27s 592ms/step - loss: 0.0477 - accuracy: 0.9304 - val_loss: 0.5261 - val_accuracy: 0.5317\n"
     ]
    }
   ],
   "source": [
    "result = model.fit(X_train, y_train, epochs=15, verbose=True, validation_data=(X_test, y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the above results number of epochs = 7\n",
    "result = model.fit(X_train, y_train, epochs=7, verbose=False, validation_data=(X_test, y_test), batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Evaluation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.1000, Precision: 0.5123, Recall: 0.5605, F1-measure: 0.5353\n",
      "Threshold: 0.2000, Precision: 0.5280, Recall: 0.5374, F1-measure: 0.5326\n",
      "Threshold: 0.2500, Precision: 0.5327, Recall: 0.5285, F1-measure: 0.5306\n",
      "Threshold: 0.3000, Precision: 0.5351, Recall: 0.5238, F1-measure: 0.5294\n",
      "Threshold: 0.4000, Precision: 0.5450, Recall: 0.5125, F1-measure: 0.5283\n",
      "Threshold: 0.5000, Precision: 0.5521, Recall: 0.5012, F1-measure: 0.5254\n",
      "Threshold: 0.6000, Precision: 0.5634, Recall: 0.4913, F1-measure: 0.5249\n",
      "Threshold: 0.7000, Precision: 0.5704, Recall: 0.4786, F1-measure: 0.5205\n",
      "Threshold: 0.8000, Precision: 0.5782, Recall: 0.4678, F1-measure: 0.5172\n",
      "Threshold: 0.9000, Precision: 0.5971, Recall: 0.4527, F1-measure: 0.5150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "thresholds=[0.1,0.2,0.25,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for val in thresholds:\n",
    "    pred=y_pred.copy()\n",
    "  \n",
    "    pred[pred>=val]=1\n",
    "    pred[pred<val]=0\n",
    "  \n",
    "    precision = precision_score(y_test, pred, average='micro')\n",
    "    recall = recall_score(y_test, pred, average='micro')\n",
    "    f1 = f1_score(y_test, pred, average='micro')\n",
    "   \n",
    "\n",
    "    print(\"Threshold: {:.4f}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(val, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion: anger, Precision: 0.3824, Recall: 0.3514, F1-measure: 0.3662\n",
      "Emotion: disgust, Precision: 0.3939, Recall: 0.3250, F1-measure: 0.3562\n",
      "Emotion: fear, Precision: 0.2353, Recall: 0.3636, F1-measure: 0.2857\n",
      "Emotion: joy, Precision: 0.6590, Recall: 0.7199, F1-measure: 0.6881\n",
      "Emotion: sadness, Precision: 0.4865, Recall: 0.4030, F1-measure: 0.4408\n",
      "Emotion: surprise, Precision: 0.3046, Recall: 0.2400, F1-measure: 0.2685\n",
      "Emotion: neutral, Precision: 0.5198, Recall: 0.5629, F1-measure: 0.5405\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###### Using this threshold of 0.2, we can determine the F1 score for each individual emotion:\n",
    "column_names = list(data.columns[6:])\n",
    "threshold = 0.2\n",
    "f1_scores = []\n",
    "for i in range(0,7):\n",
    "    emotion_prediction = y_pred[:,i]\n",
    "    emotion_prediction[emotion_prediction>=threshold]=1\n",
    "    emotion_prediction[emotion_prediction<threshold]=0\n",
    "    emotion_test = y_test[:,i]\n",
    "    precision = precision_score(emotion_test, emotion_prediction)\n",
    "    recall = recall_score(emotion_test, emotion_prediction)\n",
    "    f1 = f1_score(emotion_test, emotion_prediction)\n",
    "    print(\"Emotion: {}, Precision: {:.4f}, Recall: {:.4f}, F1-measure: {:.4f}\".format(column_names[i], precision, recall, f1))\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "plt.bar(column_names[0:7],f1_scores)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-Score across Multi-Lables: 0.5326492537313433\n"
     ]
    }
   ],
   "source": [
    "pred = y_pred.copy()\n",
    "pred[pred>=0.2] = 1\n",
    "pred[pred<0.2] =0\n",
    "print(\"Average F1-Score across Multi-Lables: {}\".format(f1_score(y_test, pred, average='micro')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
